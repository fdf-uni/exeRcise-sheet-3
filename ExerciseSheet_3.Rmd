---
title: "Exercise #3"
subtitle: "Fortgeschrittene Statistische Software für NF"
# Redact author name for privacy
author: "`r xfun::file_string('author.txt')`"
date: "`r Sys.Date()`"
output: distill::distill_article
---

```{css, echo = FALSE}
d-article p {
  text-align: justify;
}

d-article li {
  text-align: justify;
  word-wrap:break-word;
}

details summary > * {
  display: inline;
}
```

## General Remarks

-   You can submit your solutions in teams of up to 3 students.
-   Include all your team-member's names and student numbers
    (Martrikelnummern) in the `authors` field.
-   Please use the exercise template document to work on and submit your
    results.
-   Use a level 2 heading for each new exercise and answer each subtask
    next to its bullet point or use a new level 3 heading if you want.
-   Always render the R code for your solutions and make sure to include
    the resulting data in your rendered document.
    -   Make sure to not print more than 10 rows of data (unless
        specifically instructed to).
-   Always submit both the rendered document(s) as well as your source
    Rmarkdown document. Submit the files separately on moodle, **not**
    as a zip archive.

## Exercise 1: Initializing git (4 Points)

For this whole exercise sheet we will be tracking all our changes to it
in git.

a)  Start by initializing a new R project with git support, called
    `2024-exeRcise-sheet-3`. If you forgot how to do this, you can
    follow this
    [guide](https://malikaihle.github.io/Introduction-RStudio-Git-GitHub/rstudio_project.html).
b)  Commit the files generated by Rstudio.
c)  For all of the following tasks in this exercise sheet we ask you to
    always commit your changes after finishing each subtask e.g. create
    a commit after task *1d*, *1e* etc.

> Note: This applies only to answers that have text or code as their
> answer. If you complete tasks in a different order or forget to commit
> one, this is no problem. If you change your answers you can just
> create multiple commits to track the changes.

d)  Name 2 strengths and 2 weaknesses of git. (Don't forget to create a
    commit after this answer, see *1c*)
    
    **Strengths:**
    - Git is extremely fast. This is for example due to it being written in `C`,
      a very fast, low level language or it running mostly locally so it isn't
      being slowed down by having to (always) communicate with a centralized
      server.
    - Git is free and open-source (FOSS) software licensed under the GPLv2.
      Apart from increasing transparency this also makes it (and its source code)
      available to many people which also ensures that bugs are not only
      recognized but also fixed very fast resulting in it being highly reliable
      software.
    
    (Note that Git also comes with many other advantages that a version control
    system generally provides like being able to track changes in an efficient
    way. However, in the above we focused on strengths which Git has over other
    SCM tools.)
    
    **Weaknesses:**
    - Git has a rather steep learning curve. Especially for new users it can be
      a challenge to get started with Git.
      This is perhaps further amplified by there not being a native GUI (although
      there are many good third-party ones) such that some users might have to
      interact with the command line for the first time when learning how to use
      Git.
    - Since Git was mostly designed for use with text files (to be precise, 
      source code of the Linux kernel), it can struggle with binary and media
      files by using up a lot of storage.
      However, with solutions like `git-lfs` or `git-annex` and storage costs
      continuing to drop, this weakness becomes less and less severe.
      
e)  Knit this exercise sheet. Some new files will automatically be
    generated when knitting the sheet e.g. the HTML page. Ignore these
    files, as we only want to track the source files themselves.

## Exercise 2: Putting your Repository on GitHub (3.5 Points)

For this task you will upload your solution to GitHub.

a)  Create a new repository on GitHub in your account named
    `exeRcise-sheet-3`. Make sure you create a **public repository**, so
    we are able to see it for grading. Add the link to the repository
    below:
    
    <div style="text-align: center;"><https://github.com/fdf-uni/exeRcise-sheet-3></div>
    
b)  Push your code to this new repository by copying and executing the
    snippet on GitHub listed under
    `…or push an existing repository from the command line`.
c)  Regularly push your latest changes to GitHub again and especially do
    so when you are finished with this sheet.

## Exercise 3: Baby-Names in Munich (3.5 Points)

Download the latest open datasets on given names ("Vornamen") from the
open data repository of the city of Munich for the years `2023` and
`2022`.

Link: <https://opendata.muenchen.de/dataset/vornamen-von-neugeborenen>

a)  Download the data for both years and track it in git. For small
    datasets like these adding them to git is not a problem.
    
    We stored the datasets in a new directory called `data`.
    Although they're small, we still used `git-lfs` as follows:
    ```bash
    git lfs install
    git lfs track "*.csv"
    git add .gitattributes
    git add data
    ```
    Then we committed the changes.

b)  Load the data for both years into R. Check the type of the count
    variable ("Anzahl") and look into the data to determine why it is
    not numeric? Fix the problem in an appropriate manner, it is OK if
    some of the counts are inaccurate because of this. Explain your
    solution and the repercussions.
    
    We first load the datasets into `R`.
    ```{r}
    # Load the tidyverse
    library(tidyverse)
    
    # Load datasets (and clean up column names)
    names22 <- janitor::clean_names(read_csv(
      file.path("data", "given-names-munich-2022.csv"),
      show_col_types = FALSE
    ))
    names23 <- janitor::clean_names(read_csv(
      file.path("data", "given-names-munich-2023.csv"),
      show_col_types = FALSE
    ))
    ```
    
    We now check the type of the specified column.
    ```{r}
    typeof(names22$anzahl)
    typeof(names23$anzahl)
    ```
    
    In both cases, the column has been imported as a character.
    Inspection of its values shows why:
    ```{r}
    check_why_not_numeric <- function(data_set){
      # Filter all observations which contain non-numeric characters
      data_set %>% filter(str_detect(anzahl, "[^\\d]")) %>%
        # Get the attained values
        select(anzahl) %>% unique()
    }
    check_why_not_numeric(names22)
    check_why_not_numeric(names23)
    ```
    One sees that names which ocurred less than 4 times were all grouped together
    under the label "4 oder weniger" which (obviously) couldn't be interpreted
    as a number.
    
    We fix this problem by generating random samples according to
    [**Benford's law**](https://en.wikipedia.org/wiki/Benford's_law)
    (or rather its generalization to number systems of other bases than $10$,
    in this case $5$) for the observations in which the count variable has value
    "4 oder weniger".
    This of course also doesn't guarantee accuracy of the generated data since
    the distribution of given names might still be completely different, however
    it at least seems better than the following (and probably also most other)
    alternatives:
    - Use a normal distribution instead of the one from Benford's law.  
      (**Disadvantage**: Isn't really common for distribution of _digits_ in
      real-life sets of numerical data.)
    - Set all values to the average of all $4$ possible values, i.e.
      $\frac{5}{2}$ (since $\frac{1+2+3+4}{4}=\frac{10}{4}=\frac{5}{2}$).  
      (**Disadvantage**: In the end pretty similar to the one before.)
    - Set all values to maximal possible one, i.e. $4$.  
      (**Disadvantage**: Although this might seem like a good idea when one is
      interested in upper bounds and not worried about overcounting, this doesn't
      seem like the best choice with regards to the upcoming exercises, since we
      for example want to compare the number of births over the two years.)
    - Set all values to the minimal possible one, i.e. $1$. (Note that it isn't
      zero since we can expect that names which weren't given at all aren't
      included in the dataset.)  
      (**Disadvantage**: This might be of interest if one for example is
      interested in the, say, $50$ most popular given names. However for getting
      an estimate of births like in the next exercise, this is obviously
      horrible.)
    - Set all values to `NA`.  
      (**Disadvantage**: More or less the same as the one before if not even
      worse.)
    
    ```{r}
    # Create samples of given length according to Benford's law in base b
    # (we only need b=5 and n=1, but keep it like this for generality/fun).
    # The formula for the probability is from the above linked Wikipedia article.
    benf <- function(b, n){
      sample(1:(b-1), n, replace=TRUE, prob=log(1+1/(1:(b-1)), b))
    }
    
    new_anzahl <- function(data_set){
      # Ensure creation of random number for every row by using `rowwise()`
      data_set %>% rowwise() %>% 
        mutate(
          # Either replace by random value or turn into integer
          anzahl = if_else(
            anzahl == "4 oder weniger",
            benf(5,1),
            # We suppress warnings since they're only signaled due to values
            # which we don't use anyways being turned into NA's.
            suppressWarnings(as.integer(anzahl))
          )
        )
    }
    
    # Update `anzahl` column in both datasets using above function
    names22 <- new_anzahl(names22)
    names23 <- new_anzahl(names23)
    ```

c)  Calculate the total number of babies born in Munich in 2022
    and 2023. Which year had the bigger baby-boom?
    
    Assuming that our assumption from Exercise 1b) isn't too far away from
    reality we can calculate the total number of babies born in Munich in 2022
    and 2023, respectively, as follows:
    ```{r}
    sum(names22$anzahl)
    sum(names23$anzahl)
    ```
    Here we simply summed over all counts of names.
    Since compared to 2022, $`r sum(names22$anzahl) - sum(names23$anzahl)`$ less
    babies were born in 2023, we can say that (under our assumptions) 2022 had
    the bigger baby-boom.
    
    We quickly note that, of course, our evaluation of this exercise depends on
    the randomly generated data such that the conclusion might change with
    different executions of the code.
    One could therefore raise the question how often this can be expected to
    happen, however as turns out, this is in fact **extremely** unlikely:
    
    <details>
    <summary>Detailed description^[I do realize that this part isn't really
    necessary, however I was rather intrigued by the above question and therefore
    decided to try and answer it. After having spend a significant amount of time
    on it, I thought it wouldn't hurt to also include it in this file. Please
    excuse me if that's not the case.]</summary>
    We first get the guaranteed number of new born babies in each year, i.e. the
    sum of counts of the names which ocurred at least $5$ times:
    ```{r}
    # Reload datasets since we want the original columns
    names22_new <- janitor::clean_names(read_csv(
      file.path("data", "given-names-munich-2022.csv"),
      show_col_types = FALSE
    ))
    names23_new <- janitor::clean_names(read_csv(
      file.path("data", "given-names-munich-2023.csv"),
      show_col_types = FALSE
    ))
    
    guaranteed_number <- function(data_set){
      data_set %>% filter(anzahl != "4 oder weniger") %>%
        mutate(anzahl = as.numeric(anzahl)) %>%
        summarise(sum(anzahl)) %>% pull
    }
    
    birth_dif <- guaranteed_number(names22_new) - guaranteed_number(names23_new)
    ```
    We see that there are already $`r birth_dif`$ births more in the year 2022
    than in the year 2023 (once again note how this count is completely
    non-random).
    
    We now get to the actually challenging part where we have to calculate a
    remaining probability based on our random process.
    Before we can proceed, we repeat the well-known theorem from probability
    theory which states that the distribution of the sum of two independent
    random variables equals the convolution of the distributions of the given
    random variables.
    In the following, we in particular need the special case about the density
    function of the sum of random variables being the convolution of the single
    density functions, here always with respect to the counting measure.
    
    Now we proceed as follows: Note that in 3b) we chose a random value
    according to the following discrete probability density function for every
    name which ocurred 4 or less times :
    $$ p(d) := \begin{cases} \log_5\left(1+\frac{1}{d}\right) & \text{if } 1 \le
    d \le 4\\ 0 & \text{else} \end{cases} $$
    Since we did this in a row-wise manner and created the samples with
    replacement, the resulting random variables are independent (we already
    implicitely assumed this property at least to some extent in the earlier
    exercise).
    Furthermore, we can easily obtain the number of such random variables:
    ```{r}
    random_count <- function(data_set){
      data_set %>% filter(anzahl == "4 oder weniger") %>% nrow()
    }
    random_count22 <- random_count(names22_new)
    random_count23 <- random_count(names23_new)
    ```
    Thus, for the year 2022 we get the $`r random_count22`$ random variables
    $(X_k)_{1 \le k \le `r random_count22`}$ and for the year 2023 we get the
    $`r random_count23`$ random variables $(Y_l)_{1 \le l \le `r random_count22`}$
    which are all independent of each other.
    
    Now let
    $$ X := \sum_{k=1}^{`r random_count22`} X_k \qquad \text{and} \qquad
       Y := \sum_{l=1}^{`r random_count23`} Y_l. $$
    Note that by independence of the $X_k$ and $Y_l$, respectively, we have that
    the density function $p_X$ of $X$ is the $`r random_count22`$-fold
    convolution of $p$ with itself and the one of $Y$, $p_Y$, is given by the
    $`r random_count23`$-fold convolution of $p$ with itself. (This follows using
    induction from the theorem stated above.)
    Hence, if for any density function $f$, we recursively define $f^{\ast 1}=f$
    and $f^{\ast (n+1)} = f^{\ast n} \ast f$ for $n \ge 1$ , we have
    $$ p_X = p^{\ast `r random_count22`} \qquad \text{and} \qquad
       p_Y = p^{\ast `r random_count23`}. $$
    Luckily, this can be implemented in `R` rather easily thanks to the
    package `kSamples`.
    ```{r}
    # Calculate n-fold convolution of discrete random distribution with finite
    # support.
    conv_n <- function(supp, probs, n){
      supp_new <- supp
      probs_new <- probs
      for (i in 1:(n-1)){
        convol <- kSamples::conv(supp, probs, supp_new, probs_new)
        supp_new <- convol[,1]
        probs_new <- convol[,2]
      }
      return(convol)
    }
    
    supp <- 1:4
    probs <- log(1+1/(1:4), 5)
    
    p_X <- conv_n(supp, probs, random_count22)
    p_Y <- conv_n(supp, probs, random_count23)
    ```
    Now we are almost done. Letting $Z := X - Y$, note that the total difference
    of born babies between 2022 and 2023 is given by $Z + `r birth_dif`$ (we
    simply split it up into a random and a non-random part), hence we are
    interested in the probability
    $$ \mathbb{P}[Z + `r birth_dif` < 0] = \mathbb{P}[Z < - `r birth_dif`] $$
    since this is the probability that (strictly) less babies were born in 2022
    than in 2023.
    Once again using the above result about sums of independent random variables,
    we have that the density function $p_Z$ of $Z$ is given as the convolution
    of $p_X$ and the density function of $-Y$ (since $Z = X-Y = X + (-Y)$).
    Luckily, in our `R` implementation, we can easily calculate the probability
    distribution of $-Y$ based on the one of $Y$ by simply multiplying the
    corresponding support vector with $-1$.
    Hence, we can finish this analysis as follows:
    ```{r}
    probability <- as.data.frame(
      kSamples::conv(p_X[,1], p_X[,2], -p_Y[,1], p_Y[,2])
    ) %>% filter(V1 < - birth_dif) %>% summarise(sum(V2)) %>% pull
    ```
    </details>
    
    Overall, we see that under our assumptions, the probability of there having
    been more births in 2023 than in 2022  amounts to only $`r probability*100`\%$
    which is more than small enough for any reasonable measure.

d)  Add a new column `year` to both datasets which holds the correct
    year for each.
    
    We can simply proceed as follows by using `mutate()`:
    ```{r}
    names22 <- names22 %>% mutate(year = 2022)
    names23 <- names23 %>% mutate(year = 2023)
    ```
    
e)  Combine both datasets into one using `bind_rows()`.

    ```{r}
    names_combined <- bind_rows(names22, names23)
    ```

f)  Combine the counts for same names to determine the most popular
    names across both years. Print out the top 10 names in a nicely
    formatted table for both years. Include a table caption.
    
    ```{r}
    names_combined %>% group_by(vorname) %>%
      summarise(frequency = sum(anzahl), least = min(anzahl)) %>% 
      arrange(desc(frequency)) %>% head(n=10) %>% 
      knitr::kable(
        col.names = c(
          "Given name",
          "Frequency (both years)",
          "Least occurrences within a single year"
        ),
        caption = "Most popular given names in Munich across 2022 and 2023."
      )
    ```
    
    We quickly note that this table is completely independent of the random
    numbers we generated in 3b) since none of the above names appear less than
    $5$ times in either 2022 or 2023 as can be seen in the third column of the
    table (which we also added only for this purpose - it is however interesting
    to see that sorting by it instead would at least change the order of the
    names).

## Exercise 4: Open Analysis (4 points)

This exercise is a bit more open-ended. You can choose any dataset from
[Our World in Data](https://ourworldindata.org/) and analyze it, while
determining the research question yourself.

a)  Go to <https://github.com/owid/owid-datasets/tree/master/datasets>
    and choose a dataset that interests you. You can have a look at
    <https://ourworldindata.org/> to gather some inspiration.
b)  Download the dataset and track it in git.
c)  Put the name / title of the dataset and a link to it below.

    -   Dataset Name: Significant volcanic eruptions (NGDC-WDS)
    -   Link: <https://github.com/owid/owid-datasets/blob/a24e0de2c034fe13140aab88ef7579a150151ce2/datasets/Significant%20volcanic%20eruptions%20(NGDC-WDS)/Significant%20volcanic%20eruptions%20(NGDC-WDS).csv>

    (The link is perhaps longer than it would have to be, but like this we have
    a permalink so that even after possible future changes one can still access
    the same dataset.)

d)  Come up with a (research) question you want to answer with the data
    and briefly explain why you believe this is an interesting question
    within one sentence. It should be a question that can be answered
    with the dataset and using R.
    
    In the following I want to analyze which countries have been affected the
    most by significant volcanic eruptions since 1900 by counting the number
    of such eruptions for all available countries.
    In my humble opinion, this question is interesting since it might help with
    disaster preparedness and with identifying countries which might more
    probably require international help in extreme cases.
    
e)  Use R to answer your chosen question.

    ```{r}
    # Load the dataset into R (and clean up the column names)
    volcanoes <- janitor::clean_names(read_csv(
      file.path("data", "volcanic-eruptions.csv"),
      show_col_types = FALSE
    # Clean up the names a bit more
    )) %>% rename(
      region = "entity",
      number = "number_of_significant_volcanic_eruptions_ngdc_wds"
    )
    
    # Create new dataset counting number of eruptions since 1900 for every
    # country (we exclude the aggregated values for the entire world)
    eruptions <- volcanoes %>% filter(year >= 1900, region != "World") %>%
      group_by(region) %>%
      summarise(sum = sum(number))
    
    eruptions %>% arrange(desc(sum)) %>% head(n = 10) %>% 
      knitr::kable(
        col.names = c(
          "Country",
          "Number of significant volcanic eruptions"
        ),
        caption = "Top ten countries with highest number of significant volcanic
        erruptions since 1900"
      )
    ```

    The above table shows the ten countries with the most volcanic eruptions,
    answering our question from above.

f)  Create a meaningful plot / figure with the dataset. Make sure to
    provide a figure caption (via the chunk options / Rmarkdown) and
    correctly label the figure.
    
    ```{r fig.align = "center", fig.cap = "World map with number of severe volcanic eruptions"}
    # Get map data
    world <- map_data("world")
    
    # Analysis as to why some countries like the USA are not plotted correctly
    # anti_join(
    #   eruptions %>% select(region) %>% unique(),
    #   world %>% select(region) %>% unique()
    # )
    
    # Fix for this problem (this is why country codes are a thing ;) )
    eruptions <- eruptions %>% mutate(
      region = case_when(
        region == "United States" ~ "USA",
        region == "Democratic Republic of Congo" ~ "Democratic Republic of the
        Congo",
        region == "Saint Kitts and Nevis" ~ "Saint Kitts",
        region == "Saint Vincent and the Grenadines" ~ "Saint Vincent",
        region == "Trinidad and Tobago" ~ "Trinidad",
        TRUE ~ region
      )
    )
    
    # Create plot
    left_join(world, eruptions, by = "region") %>% ggplot(
      mapping = aes(
        x = long,
        y = lat,
        map_id = region,
        fill = sum
      )
    ) + geom_map(
      map = world,
      colour = "grey20",
      linewidth = 0.1,
    ) + coord_map(
      "mercator",
      xlim = c(-180,180)
    ) + scale_fill_gradientn(
      name = "Number of\neruptions",
      colours = c("forestgreen", "#D0E562", "firebrick1"),
      values = c(0, 0.15, 1),
      na.value = "grey"
    ) + theme_void(
    ) + theme(
      panel.background = element_rect(fill = "#96d0f5", color = "black"),
    ) + labs(
      title = "Severe volcanic eruptions around the world",
      subtitle = "Counting the total number of eruptions from 1900 until 2017",
      caption = paste(
        "Source: OWID dataset",
        "\"Significant volcanic eruptions (NGDC-WDS)\""
      )
    )
    ```
    
    One can clearly see our table from earlier represented in the above map.
    We quickly note that countries for which no volcanic eruption data are
    available have been filled in grey.
    Another interesting point to raise is that one can see the increase of
    eruptions around collision points of tectonic plates.
    Especially some parts of the
    [Pacific Ring of Fire](https://en.wikipedia.org/wiki/Ring_of_Fire)
    stand out with countries like the Philippines, Japan, Indonesia, USA, Chile,
    etc. having had many severe eruptions within the last century (as we already
    found out earlier in exercise 4e)).

## Final Note

Make sure to push all your commits and changes to GitHub before
submitting the exercise sheet.
